## Random binary signal (RBS)
- A continuous-time random binary signal, also called a _random telegraph signal_ due to the similarity
- It can only take 2 states, which can _change randomly_
- In comparison with a random signal with _continuous amplitude_ it has some _advantages:_
	- Has the highest _PSD_ among all the signals with bounded amplitude. A boundad amplitude is _required for a high-quality model_
	- They are _very simple to generate_, we can just use a pair of relays

### RBS autocorrelation
- We assume that the probability of $n$ sign changes $a \rightarrow -a | -a \rightarrow a$ in a given period of time $\Delta t$ is _Poisson-distributed_ with: $$P(n) = \frac{(\mu \Delta t)^n}{n!}e^{-\mu \Delta t}$$where $\mu$ is *the average number of sign changes over a time unit*
- Th probabilitieas of sign changes in $\Delta t$ are therefore:
	- 0 sign changes: $P(0) = e^{-\mu \Delta t}$
	- 1 sign change: $P(1) = \mu \Delta t e^{-\mu \Delta t}$
	- 2 sign changes: $P(n) = \frac{(\mu \Delta t)^n}{2!}e^{-\mu \Delta t}$
	- $\vdots$
- The product of $u(t)u(t+\tau)$ for RBS can either be $=a^2$ or $-a^2$
$$E\{u(t)u(t+\tau)\} = +a^2 \quad at \quad \tau=0$$the expected value of the product of a signal and it's time-shifted variant

- If the *autovariance* of the noise looks like a *delta impulse*, it's called __white noise__ 
- Sign changes are random, but mathematical expectation of $u(t)u(t+\tau)$ can be obtained *based on the probabilityof the events:*(we can see how a different set of probabilities is valid for even nd odd numbers)
$E\{u(t)u(t+\tau)\} = +a^2[P(0) + P(2) + \cdots] -a^2[P(1) + P(3) + \cdots] =$ $$a^2e^{-\mu|T|}\left[1-\frac{\mu|\tau|}{1!}+\frac{(\mu|\tau|)^2}{2!}-\frac{(\mu|\tau|)^3}{3!}+\cdots\right] = a^2e^{-2\mu|\tau|}$$wich is the _Taylor series expansion of the exponential function_
- We call this noise _broadband noise_, which is __white noise through a low-pass filter__
![[Pasted image 20250303092635.png]]

## Discrete random binary signal (DRBS)
- An approximation of RBS, makes implementation easier
- Solves the problem of possibly extremely fast changes in RBS
- Sign changes can _only occur in discrete moments_ $t = kT;k=1,2,3,\cdots$ where $T$ is the _sampling period_
![[Pasted image 20250303093420.png]]

### Autocorrelation function of DRBS
- Can be obtained by the following considerations:
	- For zeroshift $\tau=0$ only ositive products $a^2$ are obtained (because the mean value is $a^2$), which leads to $\phi_{nn}(\tau)|_{\tau = 0} = a^2$ 
	- For small shifts $(|\tau| < T)$, negative products $-a^2$ are also obtained (*their direction is proportional to the shift length*)
	- For $|\tau| \geq T$, both positive and negative products have the same probability because the shifted sisgnal is completely independent of its unshifted version for large shifts, which leads to $\phi_{nn}(\tau)|_{|\tau| \geq T} = 0$
	$$\phi_{nn}(\tau) = \begin{cases} a^2 \left[ 1 - \frac{|\tau|}{T} \right] & |\tau| < T \\ 0 & |\tau| \geq T \end{cases}$$the generalised _autocorrelation function of DRBS_

- The PSD of DRBS can be obtained by finding the _fourier transform of its autocorrelation function_ $$\phi_{nn}(\omega) = a^2T\left(\frac{\sin{\frac{\omega T}{2}}}{\frac{\omega T}{2}} \right)^2$$
![[Pasted image 20250303094624.png]]


### Pseudo-random binary signal (PRBS)
- It can be implemented with a _shift register of infinite length with a feedback_
	- A shift register with $n$ stages which can store binary information
	- The content of the register is shifted at each clock cycle
	- The input to the first stage is generated by feeding back some stages and applying an $XOR$ operation on them
	- A periodic sequence with a period $N>n$ is obtained at the putput
![[Pasted image 20250303095149.png]]
- At each cycle, a new bit is obtained at the putput of the reguster:
	- The first $n$ bits stored in the register (initial state)
	- *Feedback-generated* sequence
	- In case of proper feedback generation, all the variations of zeros and ones are obtained in the register
	- There is one *forbidden combination*, where both inputs to XOR are _zeros_
	- After that, the sequence is repeated
	- The period of PRBS is therefore $2^n -1$ 

| Number of stages (n) |  Stages for feedback (inputs to XOR)  | Period length (N) |
| :------------------: | :-----------------------------------: | :---------------: |
|          2           |                1 and 2                |         3         |
|          3           |          1 and 3 or 2 and 3           |         7         |
|          4           |          3 and 4 or 1 and 4           |        15         |
|          5           |          2 and 5 or 3 and 5           |        31         |
|          6           |                5 and 6                |        63         |
|          7           |                4 and 7                |        127        |
|          8           | full period not obtained (with 1 XOR) |         -         |
|          9           |                5 and 9                |        511        |
|          10          |               7 and 10                |       1023        |
|          11          |               10 and 11               |       2047        |

- Any stage can be used for signal generation:
	- PRBS value is $+a$ if the stage is 1
	- PRBS value is $-a$ if the stage is 0
- Properties of a period in PRBS:
	- $\frac{1}{2} \cdot \frac{N+1}{2}$ impulses of length $T$ (one half of amplitude $+a$, the other $-a$)
	- $\frac{1}{4} \cdot \frac{N+1}{2}$ impulses of length $2T$ (one half of amplitude $+a$, the other $-a$)
	- $\frac{1}{8} \cdot \frac{N+1}{2}$ impulses of length $3T$ (one half of amplitude $+a$, the other $-a$)
	- $\dots$
	- 1 negative impulse of length $(n-1)T$
	- 1 positive impulse of length $nT$
	- In total: $\frac{1}{2} (N + 1)$ of signal values $+a$ and $\frac{1}{2} (N - 1)$ of values $-a$
	- Its mean value is therefore: $E\{u(k)\} = \bar{u}(k) = \frac{a}{N}$

### Autocorrelation of PRBS
- PRBS is a very good *approxiamtion of white noise*, because all the frequencies have the same power, except for $f=0$ 


## The concepts of bias and consistency
- The bounds in many integrals and summations are _infinite_
- In practice, finite bounds are used instead 
- Two questions arise if stochastic disturbances are present:
	1. Is the mathematical expectation of the estimate _the same as the true value?_ If so, the estimate is __bias-free__. The estimate is _consistent_ if it improves with time and converges to the true value *as the interval of observation becomes infinite*
	2. Does the variance of the estimation error converge to 0 as the _observation time converges to infinity?_ If so and the estimate is _consistent in the mean square sense_. This means that *with growing observation time, the accuracy and precision of the estimate become excellent*
- __Example estimation of the mean value:__
	- Mean value of a stochastic variable $x$ is defined as: $$\overline{x} = E\{x(k)\} = \lim_{N\rightarrow \infty} \frac{1}{N}\sum_{k=1}^N{x(k)}$$
	- Is estimated using the formula: $$\hat{x} = \frac{1}{N}\sum_{k=1}^N{x(k)}$$
	- Is the estimate $\hat{x}$ of $\overline{x}$ bias free? Is it consistent? Is it consistent in the mean square?
	- Mathematical expectation of the mean value estimate $\hat{x}$ is:$$E\{\hat{x}\} = E\left\{\frac{1}{N} \sum_{k=1}^{N} x(k) \right\} = \frac{1}{N} \sum_{k=1}^{N} E\{x(k)\} = \frac{1}{N} N \bar{x} = \bar{x}$$
	- Mathematical expectation of the estimate $\hat{x}$ is the same as the true value $\overline{x}$ even in the case of finite observation inteval $N$
	- The estimate is therefore __bias-free__ and consequently _consistent_
	- The variance of the estimation error: $$E\{(\hat{x}-\overline{x})^2\} = E\{[\frac{1}{N}\sum_{k=1}^N[\hat{x}(k)-\overline{x}]]^2\}$$
	- If the individual observations $x(k)$ of the random variable are _mutualy statisticaly independent_ (white noise), the variance can be simplified:$$\begin{gather} E\{(\hat{x} - \bar{x})^2\} = E \left\{ \frac{1}{N} \sum_{k=1}^{N} (x(k) - \bar{x})^2 \right\} \\ = \frac{1}{N} \sum_{k=1}^{N} E\{(x(k) - \bar{x})^2\} \\ = \frac{1}{N} \sum_{k=1}^{N} \sigma_x^2 = \frac{1}{N} N \sigma_x^2 = \frac{1}{N} \sigma_x^2 \end{gather}$$
	- The variance of the estimation error decreases with increasing number of measurements and converges to 0 in the limit case
	- The estimate is therefore __consistent in the mean square__
- __Example: estimation of the variance:__
	- Variance of a stohastic value $x$ is calculated as:$$\hat{\sigma}_x^2=\frac{1}{N}\sum_{k=1}^N[x(k)-\hat{\overline{x}}]^2$$
	- Is $\hat{\sigma}_x^2$ the bias-free estimate of the true variance? Is it consistent?
	- Mathematical expectation of the variance estimation: $$E\{\hat{\sigma}_x^2\} = E\left\{\frac{1}{N}\sum_{k=1}^N(x(k)-\hat{\overline{x}})^2\right\}$$
	- The following holds: $$\begin{gather} \sum_{k=1}^{N} (x(k) - \hat{x})^2 = \sum_{k=1}^{N} \left[ (x(k) - \bar{x}) - (\hat{x} - \bar{x}) \right]^2 \\ = \sum_{k=1}^{N} [x(k) - \bar{x}]^2 + \sum_{k=1}^{N} (\hat{x} - \bar{x})^2 - 2 (\hat{x} - \bar{x}) \sum_{k=1}^{N} [x(k) - \bar{x}] \\ = \sum_{k=1}^{N} [x(k) - \bar{x}]^2 + N (\hat{x} - \bar{x})^2 - 2 (\hat{x} - \bar{x}) N (\hat{x} - \bar{x}) \\ = \sum_{k=1}^{N} [x(k) - \bar{x}]^2 - N (\hat{x} - \bar{x})^2 \end{gather}$$
	- And:$E\{\hat{\sigma}_x^2\} =\frac{1}{N}[\sum_{k=1}^NE\{[x(k)-\overline{x}]^2\}-NE\{(\hat{\overline{x}}-\hat{x})^2\}]=\sigma_x^2-\frac{1}{N}\sigma_x^2$ 
	- The estimate is __biased__ (for finite $N$), but __consistent__

## Parametric identification
- In general, methods for unknown parameter determination - a very broad pallette of approaches due to a large set of possible parametric models
- A parametric model is:
	- A mathematical relation between measured signals and parameters
	- A simple example is: $I = \frac{U}{R}$
- Structural identification:
	- System structure determination
	- How is the underlying equation obtained?
- Parametric identification methods:
	- Methods for unknown parameter determinations
- These two steps are often performed in cycles

- The problem is often rewritten as an _overdetermined system of equations:_
	- _More equations than unknowns_
	- All equations cannot be fulfilled simultaneously
	- If the error function is defined as a sum of squares of individual errors, the corresponding estimate is the _least-square_ one (LS)
- We get an analytical solution from the _quadratic cost function_
- This method is very old

- Ususaly the approximation improves by increasing the number of parameters, but this is not guaranteed
- We need __a priori knowledge__ to properly determine the structure of the system, otherwise the estimate will always be bad
- sometimes in identification problems, we only require parameter identification, with the structure already known
	- _structural identification:_ system structure determination
	- _parametric identification:_ unknown parameter estimation (like using the least square method)
	- These 2 steps are performed in cycles

## Scalar problem
- Process model: $$y_0(k) - Ku(k)$$
- Process with disturbances: $$y(k) = y_0(k) + n(k)$$
- Process used in the estimation algorithm: $$\hat{y}(k) = \hat{K}u(k)$$
- Only one measurement is _enough for estimation_ of the unknown $K$, but because of disturbances, more measurements are required
- Observation error: $$e(k) = y(k) - \hat{y} (k)$$
- Errors of __all $N$ observations:__ $$\begin{aligned} y(0) - \hat{K} u(0) &= e(0) \\ y(1) - \hat{K} u(1) &= e(1) \\ &\vdots \\ y(N-1) - \hat{K} u(N-1) &= e(N-1) \end{aligned}$$
- The cost function is the _sum of the suqared errors_$$V = \sum_{k=0}^{N-1} e^2(k) = \sum_{k=0}^{N-1} \left[ y(k) - \hat{K} u(k) \right]^2$$
- The cost function has to _be minimised for the parameter $R$_ $$\begin{gather}\frac{\partial V}{\partial \hat{K}} = -2 \sum_{k=0}^{N-1} \left[ y(k) - \hat{K} u(k) \right] u(k) = 0 \\ \hat{K} \sum_{k=0}^{N-1} u^2(k) = \sum_{k=0}^{N-1} u(k)y(k)\end{gather}$$
- Solution: $$\hat{K} = \frac{\sum_{k=0}^{N-1} y(k) u(k)}{\sum_{k=0}^{N-1} u^2(k)} = \frac{\hat{\phi}_{uy}(0)}{\hat{\phi}_{uu}(0)}$$
- The second derivative is __positive__ $\rightarrow$ __minimum of $V$ $$\frac{\partial^2 V}{\partial \hat{K}^2} = 2 \sum_{k=0}^{N-1} u^2(k) \geq 0$$
- The solution exists if the _denominator of $\hat{K}$ is not equal to $0$_ $$\begin{gather}\sum_{k=0}^{N-1} u^2(k) \neq 0 \\ \text{or} \\ \hat{\phi}_{uu}(0) \neq 0\end{gather}$$
- #TODO Scalar problem â€“ bias? (disturbance on y) (slide 92)
- The estimate is bias-free if the following is true: $$E\{n(k)u(k)\} = 0$$
	- If $u(k)$ and $n(k)$ are not correlated (their cross-covariance is 0) $$E\{n(k)u(k)\} = E\{n(k)\}E\{u(k)\}$$covariance is the _multiplication of 2 time-shifted signals_, with their means substracted 
	- if the _mean value of either $u(k)$ or $n(k)$ is $0$_ and they are not correlated, the estimate is __bias-free__

### Scalar problem - consistent in the mean square
#TODO 
variance, average power and autocorrelation with a 0 shift, are equivalent


## Vector problem
- With vectors, _several unknown parameters are estimated simultaneously_
- The process can be described by: $$y_0 = a_1f_1(u) + a_2f_2(u) + \cdots +a_nf_n(u)$$
- $y_0$ __depends linearly on__ $a_i$, allthough $f_i(u)$ can be arbitrary known functions (nonlinear)
- The above $n$ parameters _can be estimated if $n$ measurements are available_
- If more measurements exist, the estimation system is _overdetermined_, the LS parameter is used (sum of squared errors between themeasurements and the model output)

The mathematical model of a *regression polynomial* of order $q$ is: $$y_0 = K_0 + K_1 u + K_2 u^2 + \dots + K_q u^q$$this is also a model of an undisturbed system
- $N$ measurements are available: $$y_0(k) = K_0 + K_1 u(k) + K_2 u^2(k) + \dots + K_q u^q(k), \quad k = 0,1, \dots, N-1$$
- Which are *corrupted by noise:* $$y(k) = y_0(k)+n(k)$$
- The model of the process used in the estimation algorithm: $$\hat{y}(k) = \hat{K}_0 + \hat{K}_1 u(k) + \hat{K}_2 u^2(k) + \dots + \hat{K}_q u^q(k)$$
- The error is again the output error: $$e(k) = \hat{y}(k) - y(k)$$
- #TODO slide 99, 100, ... 108
- **The solution:** $$\mathbf{\hat{K} = [U^T U]^{-1} U^T y}$$

### Vector problem - bias? (disturbance on y)
### Dillemmas about (non)linearity of the parameter estimation problem
$y = ax+n$ is not a linear function , but an affine one. The criterion it violates it thet when an input changes $k$- times, the output should also change $k$-times
#TODO vse do 109


# Tags:
#identifikacija